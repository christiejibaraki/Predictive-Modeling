{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ1CVUwmCLYs"
      },
      "source": [
        " ## To use GPU in Google Colab,\n",
        " go to `Runtime` -> `Change runtime type` and select GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aAbHpta5CLYw"
      },
      "outputs": [],
      "source": [
        "# You may uncomment and use the command below to view info of the GPU.\n",
        "# !nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyTDEsq9CLYy"
      },
      "source": [
        " The following Python libraries are required for this part, and have been tested on Python 3.9 and Python 3.7.\n",
        " If you use Google Colab, PyTorch and SciPy are already installed, so you probably just want to install PyTorch Lightning.\n",
        "  - [PyTorch](https://pytorch.org/get-started/locally/) (tested with 1.10.1 and with 1.10.0)\n",
        "  - [PyTorch Lightning](https://pypi.org/project/pytorch-lightning/) (test with 1.5.8)\n",
        "  - [SciPy](https://scipy.org/install/) (tested with 1.7.3 and with 1.4.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "HdK5mn2NCLYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81f4eca-9514-40f9-f892-c77429c05195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning==1.5.8\n",
            "  Downloading pytorch_lightning-1.5.8-py3-none-any.whl (526 kB)\n",
            "\u001b[K     |████████████████████████████████| 526 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.8) (3.10.0.2)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.8) (1.10.0+cu111)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.8) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.8) (4.62.3)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 43.4 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.8) (1.19.5)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.1-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.8) (2.7.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 19.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.8) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.5.8) (3.0.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (1.43.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.8) (3.3.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.8) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.8) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.8) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.8) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.5.8) (3.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.8) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 55.0 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.8) (2.0.11)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=0e6abdb8ed50b73daa075b189b255734194bc42cecab594aec4173b15f62900c\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, future, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.1.0 future-0.18.2 multidict-6.0.2 pyDeprecate-0.3.1 pytorch-lightning-1.5.8 torchmetrics-0.7.1 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "# You may uncomment the line below to install PyTorch Lightning on Google Colab.\n",
        "!pip install pytorch-lightning==1.5.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7QQA35WCLYz"
      },
      "source": [
        " You may uncomment and run the code cell below to download the data.  Otherwise, you may download the data [here](https://drive.google.com/file/d/1thWkUj7uGOApr_dXRvMr9TsEHpo_H_2q/view?usp=sharing). \n",
        "\n",
        "If you are running this in Google Colab, make sure to upload any files you generated from the a2_sklearn code (e.g. unigram_vocab.json) into the appropriate directory here in Google Colab so it is accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qRagCOp6CLYz"
      },
      "outputs": [],
      "source": [
        "# !pip install gdown\n",
        "# !gdown --id 1thWkUj7uGOApr_dXRvMr9TsEHpo_H_2q -O sst2.zip\n",
        "# !mkdir -p data\n",
        "# !unzip sst2.zip -d data\n",
        "# !rm sst2.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKElCyc1CLY0"
      },
      "source": [
        " You may use the helper function below for feature weight analysis (1.1.2 and 1.2.2.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "NKjlXutLCLY1"
      },
      "outputs": [],
      "source": [
        "def print_important_weights(weights, words):\n",
        "    \"\"\"\n",
        "    Print importtatn pairs of weights and words.\n",
        "    # Parameters\n",
        "    weights : `Iterable`, required.\n",
        "        Weights from a learned model.\n",
        "    words : `Iterable`, required.\n",
        "        Word types of the vocabulary.  \n",
        "        It must be true that `len(weights) == len(words)`.\n",
        "    # Returns\n",
        "        `None`\n",
        "    \"\"\"\n",
        "\n",
        "    def print_pairs(pairs):\n",
        "        for weight, word in pairs:\n",
        "            print(\"{: .4f} | {}\".format(weight, word))\n",
        "\n",
        "    assert len(weights) == len(words)\n",
        "    pairs = list(zip(weights, words))\n",
        "    pairs = sorted(pairs, key=lambda x: x[0], reverse=True)\n",
        "    print(\"Most positive words:\")\n",
        "    print_pairs(pairs[:10])\n",
        "    print(\"\\nMost negative words:\")\n",
        "    print_pairs(reversed(pairs[-10:]))\n",
        "\n",
        "    pairs = list(zip(abs(weights), words))\n",
        "    pairs = sorted(pairs, key=lambda x: x[0], reverse=False)\n",
        "    print(\"\\nMost neutral words:\")\n",
        "    print_pairs(pairs[:10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K735Jv4CLY2"
      },
      "source": [
        " # PyTorch specific part\n",
        " ## 1.2.1 Build a Torch Logistic Regression Model\n",
        " Note that you will have to use files (of features and vocabularies) you created with `a2.sklearn` for the part below.\n",
        " You may reuse the code from there or just make sure the code below points to the right directory and files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H-y5l7BdCLY3"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from argparse import ArgumentParser\n",
        "from datetime import datetime\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from typing import Dict, List, Tuple, Type\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from scipy import sparse\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Logistic regression binary classification model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        \"\"\"\n",
        "        # Parameters\n",
        "        num_features : `int`, required.\n",
        "            Number of the features.\n",
        "        # Returns\n",
        "            `None`\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Hw-TODO: Add a linear layer to weight the features.\n",
        "        #          You may assign the layer to `self.linear`.\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Returns the logits of the model given features. \n",
        "        Note that model predictions should be either 0 or 1 based on a threshold.\n",
        "        # Parameters\n",
        "        features : `torch.FloatTensor`, required.\n",
        "            The tensor of features with the shape (batch_size, num_of_features)\n",
        "        # Returns\n",
        "        probs : `torch.FloatTensor`, required.\n",
        "            The tensor of probabilities with the shape (batch_size, 1) or (batch_size,)\n",
        "        \"\"\"\n",
        "        # Hw-TODO: Use `self.linear` you created in `__init__`\n",
        "        #          and appropriate nonlinearity/activation-function to compute\n",
        "        #          and return the probabilities of belonging to a class in the logistic regression.\n",
        "\n",
        "        return probs  # you will define this variable in the preceding code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-gJLWGwCLY4"
      },
      "source": [
        " ## Generic binary classifier as a Pytorch lightning module\n",
        " Run this cell and go to the logistic regression model to build the model.\n",
        " However, it may be useful for you to understand the next cell to understand how PyTorch Lightning works and get ready for your own project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3qR0C7lOCLY4"
      },
      "outputs": [],
      "source": [
        "class BinaryClassificationLModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # Save arguments to `hparams` attribute, see the doc [here](https://pytorch-lightning.readthedocs.io/en/stable/common/hyperparameters.html).\n",
        "        self.save_hyperparameters()\n",
        "        data_dir = Path(self.hparams.data_dir)\n",
        "        # The path `data_dir.joinpath(self.hparams.vocab_filename)` should point to unigram_vocab.json that you have generated with your code from a2_sklearn.ipynb\n",
        "        # You can configure the path through `args_str`.  See more info below where the class method `add_model_specific_args` is defined and where `args_str` is used.\n",
        "        self.hparams.vocab = json.load(\n",
        "            open(data_dir.joinpath(self.hparams.vocab_filename)))\n",
        "        self.hparams.vocab_size = len(self.hparams.vocab)\n",
        "\n",
        "        self.model = self.get_model()\n",
        "        self.step_count = 0\n",
        "        self.accuracy = Accuracy()\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.model(*args, **kwargs)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input = self.batch2input(batch)\n",
        "        labels = self.batch2labels(batch)\n",
        "        probs = self(**input)\n",
        "        probs = probs.squeeze()\n",
        "\n",
        "        # Hw-TODO: Given probs in shape (batch_size,)\n",
        "        #          and labels of the same shape,\n",
        "        #          compute the binary cross entropy loss.\n",
        "        #          The `probs`, for example, can be from the function call of\n",
        "        #          an instance of `LogisticRegressionModel` above.\n",
        "        loss = \n",
        "\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_acc', self.accuracy(probs, labels.int()), prog_bar=True)\n",
        "        output_dict = {'loss': loss}\n",
        "        return output_dict\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input = self.batch2input(batch)\n",
        "        labels = self.batch2labels(batch)\n",
        "        probs = self(**input)\n",
        "        probs = probs.squeeze()\n",
        "\n",
        "        # Hw-TODO: Given probs in shape (batch_size,)\n",
        "        #          and labels of the same shape,\n",
        "        #          compute the binary cross entropy loss.\n",
        "        loss = \n",
        "\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', self.accuracy(probs, labels.int()))\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input = self.batch2input(batch)\n",
        "        labels = self.batch2labels(batch)\n",
        "        probs = self(**input)\n",
        "        probs = probs.squeeze()\n",
        "\n",
        "        # Hw-TODO: Given probs in shape (batch_size,)\n",
        "        #          and labels of the same shape,\n",
        "        #          compute the binary cross entropy loss.\n",
        "        loss = \n",
        "\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', self.accuracy(probs, labels.int()))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.hparams.optimizer == 'sgd':\n",
        "            optimizer = torch.optim.SGD(self.model.parameters(),\n",
        "                                        lr=self.hparams.learning_rate)\n",
        "        elif self.hparams.optimizer == 'adam':\n",
        "            optimizer = torch.optim.Adam(self.model.parameters(),\n",
        "                                         lr=self.hparams.learning_rate)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.get_dataloader('train', self.hparams.train_batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.get_dataloader('dev', self.hparams.eval_batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self.get_dataloader('test', self.hparams.eval_batch_size, shuffle=False)\n",
        "\n",
        "    def get_model(self) -> nn.Module:\n",
        "        # To be overridden by inherited classes.\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def batch2input(self, batch: Tuple[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        # To be overridden by inherited classes.\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def batch2labels(self, batch: Tuple[torch.Tensor]) -> torch.Tensor:\n",
        "        # To be overridden by inherited classes.\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_dataloader(self,\n",
        "                       split: str,\n",
        "                       batch_size: int,\n",
        "                       shuffle: bool = False) -> DataLoader:\n",
        "        # To be overridden by inherited classes.\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @classmethod\n",
        "    def add_model_specific_args(cls, parser: ArgumentParser) -> ArgumentParser:\n",
        "        \"\"\"\n",
        "        Add arguments to the parser and return the parser.\n",
        "        See (https://pytorch-lightning.readthedocs.io/en/stable/common/hyperparameters.html)\n",
        "        for usage of this method.\n",
        "        \"\"\"\n",
        "        # Required arguments:\n",
        "        parser.add_argument('--vocab_filename',\n",
        "                            default=None,\n",
        "                            type=str,\n",
        "                            required=True,\n",
        "                            help=\"File name of the feature.\")\n",
        "        # Optional arguments:\n",
        "        parser.add_argument('--optimizer',\n",
        "                            default='adam',\n",
        "                            type=str,\n",
        "                            help=\"The optimizer to use, such as sgd or adam.\")\n",
        "        parser.add_argument('--learning_rate',\n",
        "                            default=1e-3,\n",
        "                            type=float,\n",
        "                            help=\"The initial learning rate for training.\")\n",
        "        parser.add_argument('--max_epochs',\n",
        "                            default=10,\n",
        "                            type=int,\n",
        "                            help=\"The number of epochs to train your model.\")\n",
        "        parser.add_argument('--train_batch_size', default=32, type=int)\n",
        "        parser.add_argument('--eval_batch_size', default=32, type=int)\n",
        "        parser.add_argument('--seed',\n",
        "                            type=int,\n",
        "                            default=42,\n",
        "                            help=\"The random seed for initialization\")\n",
        "        parser.add_argument('--do_train',\n",
        "                            action=\"store_true\",\n",
        "                            default=True,\n",
        "                            help=\"Whether to run training.\")\n",
        "        parser.add_argument('--do_predict',\n",
        "                            action=\"store_true\",\n",
        "                            help=\"Whether to run predictions on the test set.\")\n",
        "        parser.add_argument('--data_dir',\n",
        "                            default=\"data\",\n",
        "                            type=str,\n",
        "                            help=\"The input data dir. Should contain the training files.\")\n",
        "        parser.add_argument('--output_dir',\n",
        "                            type=str,\n",
        "                            help=(\"The output directory where the model predictions \"\n",
        "                                  \"and checkpoints will be written.\"))\n",
        "        # NOTE: Set --gpus 0 or change the default value to 0 if not using GPUS.\n",
        "        # See this [link](https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu.html) for usage of this argument.\n",
        "        parser.add_argument('--gpus',\n",
        "                            default=1,\n",
        "                            type=int,\n",
        "                            help=\"The number of GPUs allocated for this, 0 meaning none\")\n",
        "        parser.add_argument('--num_workers',\n",
        "                            default=8,\n",
        "                            type=int,\n",
        "                            help=\"Config `DataLoader` of pytorch\")\n",
        "        return parser\n",
        "\n",
        "\n",
        "def generic_train(args: argparse.Namespace,\n",
        "                  model_class: Type[pl.LightningModule]) -> Dict:\n",
        "    \"\"\"\n",
        "        Train (and optionally predict) and return dict results.\n",
        "        # Parameters\n",
        "        args : `argparse.Namespace`, required.\n",
        "            Configuration of the training and the model\n",
        "        model_class : `Type[pl.LightningModule]`, required.\n",
        "            Class of the model to be trained.\n",
        "        # Returns\n",
        "        A `dict` object containing the following keys and types.\n",
        "            trainer: `pl.Trainer`\n",
        "            model: `pl.LightningModule`\n",
        "            val_results_best: `list[dict]`\n",
        "                If `args.do_predict==True`\n",
        "            test_results_best: `list[dict]`\n",
        "                If `args.do_predict==True`\n",
        "            best_model_path: `Path`\n",
        "                Path to the checkpoint of the best model.\n",
        "        \"\"\"\n",
        "    pl.seed_everything(args.seed)\n",
        "\n",
        "    tensorboard_log_dir = Path(args.output_dir).joinpath('tensorboard_logs')\n",
        "    tensorboard_log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Tensorboard logger\n",
        "    tensorboard_logger = pl_loggers.TensorBoardLogger(\n",
        "        save_dir=tensorboard_log_dir,\n",
        "        version='version_' + datetime.now().strftime('%Y%m%d-%H%M%S'),\n",
        "        name='',\n",
        "        default_hp_metric=True)\n",
        "    # Checkpoint callback\n",
        "    checkpoint_dir = Path(args.output_dir).joinpath(tensorboard_logger.version,\n",
        "                                                    'checkpoints')\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=checkpoint_dir,\n",
        "                                                       filename='{epoch}-{val_acc:.2f}',\n",
        "                                                       monitor='val_acc',\n",
        "                                                       mode='max',\n",
        "                                                       save_top_k=1,\n",
        "                                                       verbose=True)\n",
        "\n",
        "    dict_args = vars(args)\n",
        "    model = model_class(**dict_args)\n",
        "    trainer = pl.Trainer.from_argparse_args(args,\n",
        "                                            logger=tensorboard_logger,\n",
        "                                            callbacks=[checkpoint_callback])\n",
        "\n",
        "    output_dict = {'trainer': trainer, 'model': model}\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer.fit(model=model)\n",
        "        # Track model performance under differnt hparams settings in \"Hparams\" of TensorBoard\n",
        "        tensorboard_logger.log_hyperparams(\n",
        "            params=model.hparams,\n",
        "            metrics={'hp_metric': checkpoint_callback.best_model_score.item()})\n",
        "        tensorboard_logger.save()\n",
        "\n",
        "        # Save the best model to `best_model.ckpt`\n",
        "        best_model_path = checkpoint_dir.joinpath('best_model.ckpt')\n",
        "        logger.info(f\"Copy best model from {checkpoint_callback.best_model_path} \"\n",
        "                    f\"to {best_model_path}.\")\n",
        "        shutil.copy(checkpoint_callback.best_model_path, best_model_path)\n",
        "\n",
        "        output_dict.update({\n",
        "            'trainer': trainer,\n",
        "            'model': model,\n",
        "            'best_model_path': best_model_path\n",
        "        })\n",
        "\n",
        "    # Optionally, predict on test set.\n",
        "    if args.do_predict:\n",
        "        best_model_path = checkpoint_dir.joinpath('best_model.ckpt')\n",
        "        model = model.load_from_checkpoint(best_model_path)\n",
        "        val_results_best = trainer.validate(model, verbose=True)\n",
        "        test_results_best = trainer.test(model, verbose=True)\n",
        "        print(\"Validation accuracy on the best model: {: .4f}\".format(\n",
        "            val_results_best[0]['val_acc']))\n",
        "        print(\"Test       accuracy on the best model: {: .4f}\".format(\n",
        "            test_results_best[0]['test_acc']))\n",
        "        output_dict.update({\n",
        "            'val_results_best': val_results_best,\n",
        "            'test_results_best': test_results_best,\n",
        "        })\n",
        "\n",
        "    return output_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JDTxC04CLY7"
      },
      "source": [
        " ## Binary classifier based on curated features.\n",
        " This is a subclass of the generic `BinaryClassificationLModule` defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tnTRLUp9CLY9"
      },
      "outputs": [],
      "source": [
        "class FeatureBasedBinaryClassificationLModule(BinaryClassificationLModule):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_model(self) -> nn.Module:\n",
        "        return LogisticRegressionModel(num_features=self.hparams.vocab_size)\n",
        "\n",
        "    def batch2input(self, batch):\n",
        "        return {'features': batch[0]}\n",
        "\n",
        "    def batch2labels(self, batch):\n",
        "        return batch[1]\n",
        "\n",
        "    def get_dataloader(self,\n",
        "                       split: str,\n",
        "                       batch_size: int,\n",
        "                       shuffle: bool = False) -> DataLoader:\n",
        "        # NOTE: In order to use different features, change feature_name by\n",
        "        # passing `--feature_name <feature_name>` in the training loop in\n",
        "        # the cell below, or revise the code here for correct paths if needed.\n",
        "        data_dir = Path(self.hparams.data_dir)\n",
        "        features_filepath = data_dir.joinpath(\n",
        "            f\"{split}_{self.hparams.feature_name}_features.npz\")\n",
        "        labels_filepath = data_dir.joinpath(split + \"_labels.npz\")\n",
        "        features = sparse.load_npz(features_filepath).todense()\n",
        "        labels = np.load(labels_filepath, allow_pickle=True)[\"arr_0\"]\n",
        "        dataset = torch.utils.data.TensorDataset(\n",
        "            torch.from_numpy(features).float(),\n",
        "            torch.from_numpy(labels).float())\n",
        "\n",
        "        logger.info(f\"Loading {split} features and labels \"\n",
        "                    f\"from {features_filepath} and {labels_filepath}\")\n",
        "        data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=shuffle,\n",
        "                                                  num_workers=self.hparams.num_workers)\n",
        "        return data_loader\n",
        "\n",
        "    @classmethod\n",
        "    def add_model_specific_args(cls, parser: ArgumentParser) -> ArgumentParser:\n",
        "        parser = super().add_model_specific_args(parser)\n",
        "        # Required arguments:\n",
        "        parser.add_argument('--feature_name',\n",
        "                            default=None,\n",
        "                            type=str,\n",
        "                            required=True,\n",
        "                            help=\"Name of the feature\")\n",
        "        # Optional arguments:\n",
        "        parser.add_argument('--task',\n",
        "                            default='featurebinarycls',\n",
        "                            type=str,\n",
        "                            help=\"Name of the task.\")\n",
        "        return parser\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlXPDPMjCLY9"
      },
      "source": [
        " # Training loop for the feature-based model of binary logistic regression\n",
        " You should replace `unigram_binary` in the assignment statement of `args_str =...`\n",
        " with whatever feature that you are experimented with.\n",
        " You can also configurate other options listed in the method of add_model_specific_args of\n",
        " the pytorch-lightning model `BinaryClassificationLModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1aijOr3-CLY9"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load hyperparameters\n",
        "parser = ArgumentParser()\n",
        "parser = FeatureBasedBinaryClassificationLModule.add_model_specific_args(parser)\n",
        "\n",
        "# IMPORTANT: here we reuse the unigram_vocab.json and the feature files generated from a2_sklearn.ipynb\n",
        "# you can read the get_dataloader function in FeatureBasedBinaryClassificationLModule \n",
        "# to understand how the data processing is handled\n",
        "# NOTE: You can replace `unigram_binary` in the assignment statement of `args_str =...`\n",
        "# with whatever feature that you are experimented with.\n",
        "# You can also configure other options listed in the method of add_model_specific_args of\n",
        "# the pytorch-lightning model `FeatureBasedBinaryClassificationLModule`.\n",
        "args_str = (\"--vocab_filename unigram_vocab.json --feature_name unigram_binary \"\n",
        "            \"--output_dir output/ftrlogistic --optimizer adam --do_train --do_predict \")\n",
        "\n",
        "args = parser.parse_args(args_str.split())\n",
        "\n",
        "# If output_dir not provided, a folder is generated\n",
        "if args.output_dir is None:\n",
        "    args.output_dir = str(\n",
        "        Path('output').joinpath(\n",
        "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
        "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Parsed arguments: {args}\")\n",
        "\n",
        "training_outout = generic_train(args=args,\n",
        "                                model_class=FeatureBasedBinaryClassificationLModule)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_8WgUQUCLY-"
      },
      "source": [
        " ## 1.2.2. Feature weight analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PozsmJjBCLY-"
      },
      "outputs": [],
      "source": [
        "model = training_outout['model']\n",
        "best_model_path = training_outout['best_model_path']\n",
        "data_dir = Path('data')\n",
        "\n",
        "model = model.load_from_checkpoint(best_model_path)\n",
        "weights = model.model.linear.weight.squeeze().detach().numpy()\n",
        "vocab = json.load(open(data_dir.joinpath('unigram_vocab.json')))\n",
        "print_important_weights(weights=weights, words=vocab.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba2EZBqWCLY-"
      },
      "source": [
        " # View modeling training curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WVXLlReGCLY_"
      },
      "outputs": [],
      "source": [
        "# You may uncomment and run the commands below to use Tensorboard in a notebook.\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir output/ftrlogistic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-_j4FiYCLY_"
      },
      "source": [
        " ## 3. Deep Averaging Networks (DAN)\n",
        " ### Build a Torch Model of Deep Averaging Networks (DAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e-IrS4C9CLY_"
      },
      "outputs": [],
      "source": [
        "class DeepAveragingNetworksModel(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab,\n",
        "                 vocab_size: int,\n",
        "                 word_embedding_size: int,\n",
        "                 hidden_size: int,\n",
        "                 num_intermediate_layers: int,\n",
        "                 dropout_rate: float,\n",
        "                 use_glove: bool = False):\n",
        "        \"\"\"\n",
        "        # Parameters\n",
        "        vocab : `dict[str, int]`, required.\n",
        "            A map from the word type to the index of the word.\n",
        "        vocab_size : `int`, required.\n",
        "            Size of the vocabulary.\n",
        "        word_embedding_size : `int`, required.\n",
        "            Size of word embeddings.\n",
        "        hidden_size : `int`, required.\n",
        "            Size of hidden layer or number of hidden units per layer.\n",
        "        num_intermediate_layers : `int`, required.\n",
        "            Number of intermediate layers, the arg takes 0 or greater integers.\n",
        "        dropout_rate : `float`, required.\n",
        "            Dropout rate.\n",
        "        use_glove : `bool`, optional.\n",
        "            Whether or not to use Glove embeddings instead of randomly initialized ones.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Return zero vector for input with padding_idx (0)\n",
        "        self.embedding = nn.Embedding(vocab_size, word_embedding_size, padding_idx=0)\n",
        "        if use_glove:\n",
        "            self.load_glove(vocab, word_embedding_size)\n",
        "\n",
        "        # Hw-TODO: Add the intermediate layers, output layer, dropout layer,\n",
        "        #          and activation function according to DAN.\n",
        "        #          You may find [nn.Modulelist](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)\n",
        "        #          useful to have multiple intermediate layers.\n",
        "\n",
        "    def forward(self, input_ids, lengths):\n",
        "        \"\"\"\n",
        "        # Parameters\n",
        "        input_ids : `torch.Tensor`, required.\n",
        "            Tensor of shape (batch_size, feature_length).\n",
        "            Each row is a datapoint represented by input words.\n",
        "        lengths: `torch.Tensor`, required.\n",
        "            Tensor of shape (batch_size, 1). Token length of input text.\n",
        "            Used to compute average word embeddings.\n",
        "        # Returns\n",
        "        probs : `torch.Tensor`\n",
        "            Tensor of shape (batch_size)\n",
        "        \"\"\"\n",
        "        out = self.embedding(input_ids)  # shape: (batch_sz, max_len, embedding_sz)\n",
        "\n",
        "        # Hw-TODO: Use the intermediate layers, output layer, dropout layer,\n",
        "        #          and activation function you created in __init__\n",
        "        #          and other appropriate non-linearity for the output layer\n",
        "        #          to compute the probabilies of a class, assign these probabilities\n",
        "        #          to a variable named \"probs\".\n",
        "\n",
        "        return probs # you will define this variable in the preceding code.\n",
        "\n",
        "    def load_glove(self, vocab, word_embedding_size):\n",
        "        logger.info(\"Load glove pretrained word embeddings\")\n",
        "        # Hw-TODO: [extra credit] Load glove onto self.embeddings\n",
        "        #          you may find [load_state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict) useful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br7ww6KmCLZA"
      },
      "source": [
        " ## Binary classifier based on curated features.\n",
        " `DeepAveragingBinaryClassificationLModule` is subclass of the generic `BinaryClassificationLModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "i_TfmC51CLZA"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "\n",
        "class SST2Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Using dataset to process input text on-the-fly\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.max_len = 50  # assigned based on length analysis of training set\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        note = []\n",
        "        label, text = int(self.data[index][0]), self.data[index][1]\n",
        "        tokens = self.tokenizer.tokenize(text.lower())\n",
        "        # If word does not exist, give <unk> token id\n",
        "        token_ids = [self.vocab.get(t, 1) for t in tokens]\n",
        "        length = min(len(token_ids), self.max_len)\n",
        "        # Truncate or pad to max length\n",
        "        padded_token_ids = token_ids[:50] + [0] * (self.max_len - length)\n",
        "        return padded_token_ids, length, label\n",
        "\n",
        "    def collate_fn(self, batch_data):\n",
        "        padded_token_ids, lengths, labels = list(zip(*batch_data))\n",
        "        return (\n",
        "            torch.LongTensor(padded_token_ids).view(-1, self.max_len),\n",
        "            torch.FloatTensor(lengths).view(-1, 1),\n",
        "            torch.FloatTensor(labels).view(-1, 1),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class DeepAveragingBinaryClassificationLModule(BinaryClassificationLModule):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_model(self) -> nn.Module:\n",
        "        return DeepAveragingNetworksModel(\n",
        "            vocab=self.hparams.vocab,\n",
        "            vocab_size=self.hparams.vocab_size,\n",
        "            word_embedding_size=self.hparams.word_embedding_size,\n",
        "            hidden_size=self.hparams.hidden_size,\n",
        "            num_intermediate_layers=self.hparams.num_intermediate_layers,\n",
        "            dropout_rate=self.hparams.dropout_rate,\n",
        "            use_glove=self.hparams.use_glove)\n",
        "\n",
        "    def batch2input(self, batch):\n",
        "        return {'input_ids': batch[0], 'lengths': batch[1]}\n",
        "\n",
        "    def batch2labels(self, batch):\n",
        "        return batch[2].squeeze()\n",
        "\n",
        "    def get_dataloader(self, split, batch_size, shuffle=False) -> DataLoader:\n",
        "        data_dir = Path(self.hparams.data_dir)\n",
        "        datapath = data_dir.joinpath(f\"sst2.{split}\")\n",
        "        data = open(datapath).readlines()\n",
        "        data = [d.strip().split(\" \", maxsplit=1) for d in data\n",
        "               ]  # list of [label, text] pair\n",
        "        dataset = SST2Dataset(vocab=self.hparams.vocab,\n",
        "                              data=data,\n",
        "                              tokenizer=WordPunctTokenizer())\n",
        "\n",
        "        logger.info(f\"Loading {split} data and labels from {datapath}\")\n",
        "        data_loader = DataLoader(dataset=dataset,\n",
        "                                 batch_size=batch_size,\n",
        "                                 shuffle=shuffle,\n",
        "                                 num_workers=self.hparams.num_workers,\n",
        "                                 collate_fn=dataset.collate_fn)\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.hparams.optimizer == 'sgd':\n",
        "            optimizer = torch.optim.SGD(self.model.parameters(),\n",
        "                                        lr=self.hparams.learning_rate)\n",
        "        elif self.hparams.optimizer == 'adam':\n",
        "            optimizer = torch.optim.Adam(self.model.parameters(),\n",
        "                                         lr=self.hparams.learning_rate)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # Hw-TODO: Add more optimizers and experiment with at least 2\n",
        "        #          optimizers other than vanilla SGD.\n",
        "        #          You can configure which optimizer to use by modifying\n",
        "        #          args_str or args passted to the function generic_train.\n",
        "        return optimizer\n",
        "\n",
        "    @classmethod\n",
        "    def add_model_specific_args(cls, parser: ArgumentParser) -> ArgumentParser:\n",
        "        parser = super().add_model_specific_args(parser)\n",
        "\n",
        "        # Required arguments\n",
        "        parser.add_argument('--num_intermediate_layers',\n",
        "                            type=int,\n",
        "                            help=\"number of intermediate layers\")\n",
        "        # Optional arguments\n",
        "        parser.add_argument('--dropout_rate',\n",
        "                            default=0.5,\n",
        "                            type=float,\n",
        "                            help=\"Dropout rate\")\n",
        "        parser.add_argument('--word_embedding_size',\n",
        "                            default=300,\n",
        "                            type=int,\n",
        "                            help=\"Size of word embeddings\")\n",
        "        parser.add_argument('--hidden_size',\n",
        "                            default=300,\n",
        "                            type=int,\n",
        "                            help=\"Size of hidden layer\")\n",
        "        parser.add_argument('--use_glove',\n",
        "                            action=\"store_true\",\n",
        "                            help=\"Whether to run predictions on the test set.\")\n",
        "        parser.add_argument('--task',\n",
        "                            default='danbinarycls',\n",
        "                            type=str,\n",
        "                            help=\"Name of the task.\")\n",
        "        return parser\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbF_Z9KcCLZB"
      },
      "source": [
        " # Training loop for the feature-based model of deep averaging networks.\n",
        " You can configurate other options listed in the method of add_model_specific_args of\n",
        " the pytorch-lightning model `BinaryClassificationLModule`.\n",
        " The example below trains with vanilla SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rPqTJ5GZCLZB"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "                    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load hyperparameters\n",
        "parser = ArgumentParser()\n",
        "parser = DeepAveragingBinaryClassificationLModule.add_model_specific_args(parser)\n",
        "\n",
        "# NOTE: You should replace --optimizer <optimizer> with the name of the optimizer\n",
        "# with which you are experimenting with, and the same goes for word_embedding_size.\n",
        "# You can also configure other options listed in the method of add_model_specific_args of\n",
        "# the pytorch-lightning module `DeepAveragingBinaryClassificationLModule`.\n",
        "args_str = (\"--vocab_filename unigram_vocab.json \"\n",
        "            \"--optimizer sgd --num_intermediate_layers 1 \"\n",
        "            \"--output_dir output/dan  --do_train --do_predict \")\n",
        "args = parser.parse_args(args_str.split())\n",
        "\n",
        "# If output_dir not provided, a folder is generated\n",
        "if args.output_dir is None:\n",
        "    args.output_dir = str(\n",
        "        Path('output').joinpath(\n",
        "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
        "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Parsed arguments: {args}\")\n",
        "\n",
        "training_outout = generic_train(args=args,\n",
        "                                model_class=DeepAveragingBinaryClassificationLModule)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw_3MtS0CLZB"
      },
      "source": [
        " Another example with Adam optimizer and 2 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NbwnK6k_CLZB"
      },
      "outputs": [],
      "source": [
        "# Load hyperparameters\n",
        "parser = ArgumentParser()\n",
        "parser = DeepAveragingBinaryClassificationLModule.add_model_specific_args(parser)\n",
        "\n",
        "# NOTE: You should replace --optimizer <optimizer> with the name of the optimizer\n",
        "# with which you are experimenting with, and the same goes for word_embedding_size.\n",
        "# You can also configure other options listed in the method of add_model_specific_args of\n",
        "# the pytorch-lightning module `DeepAveragingBinaryClassificationLModule`.\n",
        "args_str = (\"--vocab_filename unigram_vocab.json \"\n",
        "            \"--optimizer adam --num_intermediate_layers 2 \"\n",
        "            \"--output_dir output/dan  --do_train --do_predict \")\n",
        "args = parser.parse_args(args_str.split())\n",
        "\n",
        "# If output_dir not provided, a folder is generated\n",
        "if args.output_dir is None:\n",
        "    args.output_dir = str(\n",
        "        Path('output').joinpath(\n",
        "            f\"{args.task}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"))\n",
        "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Parsed arguments: {args}\")\n",
        "\n",
        "training_outout = generic_train(args=args,\n",
        "                                model_class=DeepAveragingBinaryClassificationLModule)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Axq_lP8VCLZB"
      },
      "outputs": [],
      "source": [
        "# You may uncomment and run the commands below to use Tensorboard in a notebook.\n",
        "# %reload_ext  tensorboard\n",
        "# %tensorboard --logdir output/dan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JecYfzP4CLZC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "a2_pytorch_skeleton.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}