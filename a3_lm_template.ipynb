{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tX81sP7J-ta"
      },
      "source": [
        " The following Python libraries are required for this part, and have been tested on Python 3.9 and Python 3.7.\n",
        " If you use Google Colab, PyTorch is already installed.\n",
        "  - [PyTorch](https://pytorch.org/get-started/locally/) (tested with 1.10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcWCOlx1vSmf"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDlJ3yEpvX4n",
        "outputId": "bb0c8d2c-442e-4821-f5f6-d4b8014ecdc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# You may prefer to upload the data to your google drive and mount your google drive to this colab, \n",
        "# because the data will be erased if you stop using this colab for a while.\n",
        "# Uncomment the code below to do so. After mounting, navigate to the appropriate folder, right click, and \"copy path\".\n",
        "# Assign DATA_DIR global variable to that path.\n",
        "# For more mounting instructions: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=XDg9OBaYqRMd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "zyBV10nMJIrO"
      },
      "outputs": [],
      "source": [
        "# If imported from google drive, config for your file directory. Mine is 'lm_data'.\n",
        "DATA_DIR = \"/content/drive/MyDrive/a3_data/lm_data\"\n",
        "\n",
        "# the goal is that DATA_DIR points to where the training/validation/test data is. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "j2IKVqkxzN1L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "7asRBEtT102s"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "TRAIN_BATCH_SIZE = 100\n",
        "TEST_BATCH_SIZE = 100\n",
        "WORD_EMBED_DIM = 200\n",
        "HID_EMBED_DIM = 200 \n",
        "N_LAYERS = 2 \n",
        "DROPOUT = 0.5 \n",
        "LOG_INTERVAL = 100\n",
        "EPOCHS = 10\n",
        "BPTT = 50 # sequence length\n",
        "CLIP = 0.25\n",
        "TIED = False\n",
        "SAVE_BEST = os.path.join(DATA_DIR, 'model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_45k6t7hzgsA"
      },
      "source": [
        "## Build vocabulary and convert text in corpus to lists of word index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "\n",
        "def read_text_file_to_list(file):\n",
        "    \"\"\"\n",
        "    Read txt file to list of string\n",
        "    Splits on newline\n",
        "    Input:\n",
        "        file: (str) filename\n",
        "    Output:\n",
        "        lines: list of (str)\n",
        "    \"\"\"\n",
        "    with open(file) as f:\n",
        "        lines = f.read().splitlines()\n",
        "        return lines"
      ],
      "metadata": {
        "id": "HE4w7zdWO_Gr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "xr3dkhUg2fiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbfeaef-8223-4ad5-b17d-ae804cbb4202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2099444\n",
            "218808\n",
            "246993\n",
            "28913\n"
          ]
        }
      ],
      "source": [
        "class WordDict(object):\n",
        "    def __init__(self):\n",
        "        # mapping between word type to its index\n",
        "        self.word2idx = {}\n",
        "        # mapping between index to word type\n",
        "        self.idx2word = {}\n",
        "        self.next_id = 0 # default value\n",
        "        self.add_word(\"<sos>\")\n",
        "        self.add_word(\"<eos>\")\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # TODO: add word to the dictionary by updating both word2idx and idx2word\n",
        "        if word not in self.word2idx:\n",
        "          self.word2idx[word] = self.next_id\n",
        "          self.idx2word[self.next_id] = word\n",
        "          self.next_id += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.train_file = read_text_file_to_list(os.path.join(path, 'train.txt'))\n",
        "        self.valid_file = read_text_file_to_list(os.path.join(path, 'valid.txt'))\n",
        "        self.test_file = read_text_file_to_list(os.path.join(path, 'test.txt'))\n",
        "\n",
        "        self.dictionary = WordDict() \n",
        "        self.add_data_to_vocab()\n",
        "\n",
        "        self.train = self.tokenize(self.train_file)\n",
        "        self.valid = self.tokenize(self.valid_file)\n",
        "        self.test = self.tokenize(self.test_file)\n",
        "    \n",
        "    def add_data_to_vocab(self):\n",
        "      \"\"\"\n",
        "      Add tokens from train, validation, and test set to vocab\n",
        "      \"\"\"\n",
        "      for data in [self.train_file, self.valid_file, self.test_file]:\n",
        "        for line in data:\n",
        "          line = re.sub('\\s+',' ',line)\n",
        "          line = line.strip()\n",
        "          if line:\n",
        "            line = line.lower()\n",
        "            for token in line.split():\n",
        "              self.dictionary.add_word(token)\n",
        "              \n",
        "    def tokenize(self, text):\n",
        "        ################################\n",
        "        ## TODO: \n",
        "        ## (1) build vocabulary on three given files, using class WordDict \n",
        "        ## (2) tokenize each file content with the vocabulary, return a list of token ids\n",
        "        ## Note that in this implementation, we add words in validation and test file into the vocabulary,\n",
        "        ## so there is no unknown word.\n",
        "        ################################\n",
        "\n",
        "        all_token_ids = []\n",
        "\n",
        "        for line in text:\n",
        "          line = re.sub('\\s+',' ',line)\n",
        "          line = line.strip()\n",
        "          if line:\n",
        "            line = line.lower()\n",
        "            line_tokens = ['<sos>'] + line.split() + ['<eos>']\n",
        "            all_token_ids.extend([self.dictionary.word2idx[token] for token in line_tokens])\n",
        "        return all_token_ids\n",
        "\n",
        "corpus = Corpus(DATA_DIR)\n",
        "print(len(corpus.train))\n",
        "print(len(corpus.valid))\n",
        "print(len(corpus.test))\n",
        "print(len(corpus.dictionary))\n",
        "assert len(corpus.dictionary) == 28913\n",
        "assert len(corpus.train) == 2099444\n",
        "assert len(corpus.valid) == 218808\n",
        "assert len(corpus.test) == 246993"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "6of33pFT94dM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46394c93-8ef2-4557-8c0c-2e9cd9314fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20994, 100])\n",
            "torch.Size([2188, 100])\n",
            "torch.Size([2469, 100])\n"
          ]
        }
      ],
      "source": [
        "def batchify(ids, batch_size):\n",
        "    \"\"\"\n",
        "    batchify arranges the dataset into columns.\n",
        "    # Parameters\n",
        "    ids : Tensor\n",
        "        1-dimensional tensor of token ids\n",
        "    batch_size : Int\n",
        "        batch_size\n",
        "    # Returns\n",
        "    data: a torch.LongTensor with shape of (batch_size, len(ids)//batch_size)\n",
        "        batchified corpus data\n",
        "\n",
        "    For example, the input ids [1,2,3,4,5,6,7,8,9] and batch_size=2\n",
        "    output is:\n",
        "    [ [1, 5],\n",
        "      [2, 6],\n",
        "      [3, 7],\n",
        "      [4, 8] ]\n",
        "    The shape of the tensor is 4x2. \n",
        "    We trim off any extra elements (9 in this example) that wouldn't cleanly fit.\n",
        "    ***Again, note that the text order is in the column.***\n",
        "    \"\"\" \n",
        "    num_cols = math.floor(len(ids)/batch_size)\n",
        "    list_subsets = [ids[i:i+num_cols] for i in \n",
        "                    range(0, len(ids)-num_cols+1, num_cols)]\n",
        "\n",
        "    batchified = torch.transpose(torch.LongTensor(list_subsets),0,1)\n",
        "    return batchified\n",
        "    \n",
        "# print(batchify([1,2,3,4,5,6,7,8,9], 2))\n",
        "# print(batchify([1,2,3,4,5,6,7,8,9], 3))\n",
        "# print(batchify([1,2,3,4,5,6,7,8,9,10], 3))\n",
        "# print(batchify([1,2,3,4,5,6,7,8,9,10], 2))\n",
        "\n",
        "train_data = batchify(corpus.train, TRAIN_BATCH_SIZE)\n",
        "val_data = batchify(corpus.valid, TEST_BATCH_SIZE)\n",
        "test_data = batchify(corpus.test, TEST_BATCH_SIZE)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIYJzPZLQPbU",
        "outputId": "e1355eb1-29c8-4dc3-9201-621232fb986b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,   701,    10,  ...,    18, 28809,   272],\n",
            "        [    2,  1791,    14,  ...,   438,  8623, 20553],\n",
            "        [    3,   130,   119,  ...,   984,    18,   300],\n",
            "        ...,\n",
            "        [  794,    18,  7030,  ...,   436,   407,    17],\n",
            "        [ 3536,  5282,    11,  ...,   843,   206,   157],\n",
            "        [ 1566,   725,  3917,  ...,    16,   290,   857]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "qhtQEbhR_hNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0da8fc0-0b42-43b5-a4e7-bc4b06b9dd7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[1, 5],\n",
            "        [2, 6]]), tensor([2, 6, 3, 7]))\n",
            "(tensor([[3, 7]]), tensor([4, 8]))\n",
            "torch.Size([50, 100])\n",
            "torch.Size([5000])\n",
            "tensor([[    0,   701,    10,  ...,    18, 28809,   272],\n",
            "        [    2,  1791,    14,  ...,   438,  8623, 20553],\n",
            "        [    3,   130,   119,  ...,   984,    18,   300],\n",
            "        ...,\n",
            "        [   35,    17,  5419,  ...,  5099,    16,    14],\n",
            "        [   36,   346,    62,  ...,    14,     1,  1625],\n",
            "        [   37,  3544,    38,  ...,  7773,     0,  1654]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ],
      "source": [
        "def get_batch(source, i, bptt = BPTT):\n",
        "    \"\"\"\n",
        "    # Parameters\n",
        "    source : Tensor\n",
        "        corpus as 2-dimensional tensor\n",
        "    i : Int\n",
        "        minibatch index\n",
        "\n",
        "    # Returns\n",
        "    data : 2D tensor \n",
        "        LSTM input\n",
        "    target : 1D tensor\n",
        "        LSTM output target\n",
        "\n",
        "    Consider the following example where \"source\" is a 2d tensor of shape (4, 2).\n",
        "    In this example we have 4 batches, each of size 2.\n",
        "    [ [1, 5],\n",
        "      [2, 6],\n",
        "      [3, 7],\n",
        "      [4, 8] ]\n",
        "\n",
        "    Suppose we set BPTT (backpropagation through time, see A3 pdf for details) to 2.\n",
        "    At index i = 0, the input to our LSTM becomes:\n",
        "    [ [1, 5],\n",
        "      [2, 6] ]\n",
        "    This corresponds to the first 2 batches in the sequence.\n",
        "    The target would correspondingly be (again, since BPTT is 2): \n",
        "    [ [2, 6],\n",
        "      [3, 7] ]\n",
        "    However, we need to reshape it from 2-dimensions to 1-dimension:\n",
        "    [2, 6, 3, 7]\n",
        "\n",
        "    For the next batch, index i = prev_i + BPTT = 2. \n",
        "    However, i + BPTT = 2 + 2 = 4 and 4 >= len(source). This wouldn't work.\n",
        "    So, to account for this edge case, we consider BPTT to be:\n",
        "    len(source) - 1 - i = 4 - 1 - 2 = 1\n",
        "    As such, our input now becomes:\n",
        "    [ [3, 7] ]\n",
        "    and target is \n",
        "    [4, 8]. \n",
        "    \"\"\" \n",
        "    ###################################################\n",
        "    # TODO Assign these variables to the right values #\n",
        "    ###################################################\n",
        "    seq_len =  (len(source) - 1 - i) if (i + bptt  >= len(source)) else bptt\n",
        "    # data = torch.unsqueeze(torch.tensor(source[i:i+BPTT]),0)\n",
        "    data = torch.tensor(source[i:i+seq_len])\n",
        "    target = torch.flatten(torch.tensor(source[i+1:i+1+seq_len]))\n",
        "    # print(data)\n",
        "    # print(target)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "test = batchify([1,2,3,4,5,6,7,8,9], 2)\n",
        "print(get_batch(test, i=0, bptt=2))\n",
        "print(get_batch(test, i=2, bptt=2))\n",
        "\n",
        "data, targets = get_batch(train_data, 0)\n",
        "print(data.shape)\n",
        "print(targets.shape)\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "6C8QNsUL9i1S"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "## TODO: Implement RNN LSTM\n",
        "## documentation of pytorch LSTM interface: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "################################\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, word_embedding_size, nhid, nlayers, dropout=0.5, tied_weights=False):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.nhid = nhid # hidden dimension of LSTM\n",
        "        self.nlayers = nlayers # number of LSTM layers\n",
        "        # TODO: initialize the required modules for the LSTM model\n",
        "        # HINT: batch_first should be False in LSTM since our data structure is not batch first.\n",
        "        self.encoder = self.embedding = torch.nn.Embedding(vocab_size, word_embedding_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.lstm = nn.LSTM(input_size = word_embedding_size, \n",
        "                          hidden_size = self.nhid, \n",
        "                          num_layers = self.nlayers,\n",
        "                          batch_first = False)\n",
        "        self.decoder = nn.Linear(self.nhid, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        For example:\n",
        "        # initrange = 0.1\n",
        "        # nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        This is not all that you need!\n",
        "        \"\"\"\n",
        "        # TODO: initialize the parameters\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)   \n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)   \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"\n",
        "        # Parameters\n",
        "        input: input embedding\n",
        "        hidden: hidden states in LSTM\n",
        "        # Returns\n",
        "        decoded: refers to the output of decoder layer over the vocabulary. Note that you don't need to pass it through the softmax layer\n",
        "        hidden: stores the hidden states in LSTM\n",
        "        \"\"\"\n",
        "        embeds = self.embedding(input)\n",
        "        # print(\"embeddings shape: \", embeds.shape)\n",
        "        out = self.dropout(embeds)\n",
        "        out, hidden = self.lstm(out, hidden)\n",
        "        # print(\"lstm out shape: \", out.shape)\n",
        "        out = self.dropout(embeds)\n",
        "        decoded = self.decoder(out).flatten(0,1)\n",
        "        return decoded, hidden\n",
        "\n",
        "    # initialize parameters in LSTM\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "            weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "hVpQfjuxOm0Y"
      },
      "outputs": [],
      "source": [
        "# Set the random seed for reproducibility.\n",
        "torch.manual_seed(SEED)\n",
        "# set device as GPU/CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "steNXz9I9iqI",
        "outputId": "e686882f-184a-4858-c2df-99c5bc0735f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "xZT5ZAu7EX3v"
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    print(\"ntokens: \", ntokens)\n",
        "    hidden = model.init_hidden(TRAIN_BATCH_SIZE)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, BPTT)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # print(torch.max(data))\n",
        "        # print(\"data shape: \", data.shape)\n",
        "        # print(\"targets shape: \", targets.shape)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        hidden = repackage_hidden(hidden) # Note that the main advantage here is that the hidden value is continual from the previous forward pass\n",
        "        output, hidden = model(data, hidden)\n",
        "        # print(\"output shape: \", output.shape)\n",
        "        # print(\"target shape: \", targets.shape)\n",
        "        # print(\"output type: \", type(output))\n",
        "        # print(\"target type: \", type(targets))\n",
        "        loss = criterion(output.to(torch.float), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
        "            cur_loss = total_loss / LOG_INTERVAL\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // BPTT,\n",
        "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "aTsdN6rLG0fc"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute the loss of model on data_source\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    # TODO: get the average negative log likelihood on the data_source\n",
        "    # return average_log_loss\n",
        "    epoch_loss, epoch_acc = 0, 0\n",
        "    epoch_tp, epoch_fp, epoch_fn = 0, 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      hidden = model.init_hidden(TEST_BATCH_SIZE)\n",
        "      batch_starts = range(0, data_source.size(0) - 1, BPTT)\n",
        "      for batch, i in enumerate(batch_starts):\n",
        "        data, targets = get_batch(data_source, i)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        out, hidden = model.forward(data, hidden)\n",
        "        loss = criterion(out.to(torch.float), targets)\n",
        "        epoch_loss += loss.item() / len(batch_starts)\n",
        "    print(\"epoch loss: \", epoch_loss)\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "MYuaQ5cSHxWI"
      },
      "outputs": [],
      "source": [
        "# prepare the model, loss, and optimizer\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = LSTMModel(ntokens, WORD_EMBED_DIM, HID_EMBED_DIM, N_LAYERS, DROPOUT, TIED).to(device)\n",
        "criterion = nn.CrossEntropyLoss() # use crossentropy loss\n",
        "optimizer = torch.optim.Adam(model.parameters()) # use adam optimizer with default setting\n",
        "best_val_loss = None\n",
        "\n",
        "# Training framework\n",
        "# for epoch in range(1, EPOCHS+1):\n",
        "#     epoch_start_time = time.time()\n",
        "#     train()\n",
        "#     val_loss = evaluate(val_data)\n",
        "#     print('-' * 89)\n",
        "#     print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "#         'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "#         val_loss, math.exp(val_loss)))\n",
        "#     print('-' * 89)\n",
        "    \n",
        "#     # Save the model if the validation loss is the best we've seen so far.\n",
        "#     if not best_val_loss or val_loss < best_val_loss:\n",
        "#         with open(SAVE_BEST, 'wb') as f:\n",
        "#             torch.save(model, f)\n",
        "#             print(\"save new best model!\")\n",
        "#         best_val_loss = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "1XxYhaWUicWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0cf07e-b106-41bb-8c3d-b17decfda783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch loss:  5.410466632843017\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.41 | test ppl   223.74\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load the best saved model.\n",
        "with open(SAVE_BEST, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # After loading the RNN params, they are not a continuous chunk of memory.\n",
        "    # flatten_paramters() makes them a continuous chunk, and will speed up the forward pass.\n",
        "    # Currently, only RNN model supports flatten_parameters function.\n",
        "    model.lstm.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "YmzWgouuHhwF"
      },
      "outputs": [],
      "source": [
        "# Generation with GPT-2\n",
        "# Check this tutorial: https://huggingface.co/blog/how-to-generate\n",
        "# It comes with a notebook. You need to run through that notebook and understand different sampling procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": true,
        "id": "aKmS7WvIHhwG"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, sampling_func):\n",
        "    # # Generation with LSTM lm given a sampling function and a prompt\n",
        "    max_length = 30\n",
        "    ids = []\n",
        "    for word in prompt.split():\n",
        "        ids.append(corpus.dictionary.word2idx[word])\n",
        "    print(ids)\n",
        "    hidden = model.init_hidden(1)\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        output, hidden = model(torch.LongTensor([[wid] for wid in ids]).to(device), hidden)\n",
        "        word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "        print(word_prob.shape)\n",
        "        generations = []\n",
        "        for i in range(max_length):\n",
        "            word_idx = sampling_func(word_prob)\n",
        "            print(word_idx)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            generations.append(word)\n",
        "            if word == \"<eos>\":\n",
        "                break\n",
        "            new_word = torch.LongTensor([[word_idx]]).to(device)\n",
        "            output, hidden = model(new_word, hidden)\n",
        "            word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "    return generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "MnEmUWiwHhwG"
      },
      "outputs": [],
      "source": [
        "def greedy_sampling(word_prob):\n",
        "    # TODO: return the word with the max probability\n",
        "    return torch.argmax(word_prob).item()\n",
        "\n",
        "def random_sampling(word_prob):\n",
        "    # TODO: sample a random word based on the probability vector\n",
        "    \n",
        "    return word_id\n",
        "\n",
        "def topk_sampling(word_prob):\n",
        "    # TODO: top k sampling as explained in the assignment\n",
        "    return word_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "XpCGmlnTHhwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cabebb-06e7-4dee-89f2-5225e1d48be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3339]\n",
            "torch.Size([28913])\n",
            "79\n",
            "158\n",
            "203\n",
            "28\n",
            "10\n",
            "14\n",
            "38\n",
            "18\n",
            "10\n",
            "14\n",
            "38\n",
            "18\n",
            "10\n",
            "14\n",
            "38\n",
            "18\n",
            "10\n",
            "14\n",
            "38\n",
            "18\n",
            "10\n",
            "14\n",
            "38\n",
            "18\n",
            "10\n",
            "14\n",
            "38\n",
            "18\n",
            "10\n",
            "14\n",
            "prompt: we \n",
            "are not be a <unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk> ,\n"
          ]
        }
      ],
      "source": [
        "prompt = \"we \".lower()\n",
        "generations = generate_text(prompt, greedy_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "a3_lm_template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}