{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tX81sP7J-ta"
      },
      "source": [
        " The following Python libraries are required for this part, and have been tested on Python 3.9 and Python 3.7.\n",
        " If you use Google Colab, PyTorch is already installed.\n",
        "  - [PyTorch](https://pytorch.org/get-started/locally/) (tested with 1.10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcWCOlx1vSmf"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDlJ3yEpvX4n",
        "outputId": "be2c53c8-c32e-4d05-9441-3577638732dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# You may prefer to upload the data to your google drive and mount your google drive to this colab, \n",
        "# because the data will be erased if you stop using this colab for a while.\n",
        "# Uncomment the code below to do so. After mounting, navigate to the appropriate folder, right click, and \"copy path\".\n",
        "# Assign DATA_DIR global variable to that path.\n",
        "# For more mounting instructions: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=XDg9OBaYqRMd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "zyBV10nMJIrO"
      },
      "outputs": [],
      "source": [
        "# If imported from google drive, config for your file directory. Mine is 'lm_data'.\n",
        "DATA_DIR = \"/content/drive/MyDrive/a3_data/lm_data\"\n",
        "\n",
        "# the goal is that DATA_DIR points to where the training/validation/test data is. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "j2IKVqkxzN1L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "7asRBEtT102s"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "TRAIN_BATCH_SIZE = 100\n",
        "TEST_BATCH_SIZE = 100\n",
        "WORD_EMBED_DIM = 200\n",
        "HID_EMBED_DIM = 200 \n",
        "N_LAYERS = 2 \n",
        "DROPOUT = 0.5 \n",
        "LOG_INTERVAL = 100\n",
        "EPOCHS = 10\n",
        "BPTT = 50 # sequence length\n",
        "CLIP = 0.25\n",
        "TIED = False\n",
        "SAVE_BEST = os.path.join(DATA_DIR, 'model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_45k6t7hzgsA"
      },
      "source": [
        "## Build vocabulary and convert text in corpus to lists of word index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "\n",
        "def read_text_file_to_list(file):\n",
        "    \"\"\"\n",
        "    Read txt file to list of string\n",
        "    Splits on newline\n",
        "    Input:\n",
        "        file: (str) filename\n",
        "    Output:\n",
        "        lines: list of (str)\n",
        "    \"\"\"\n",
        "    with open(file) as f:\n",
        "        lines = f.read().splitlines()\n",
        "        return lines"
      ],
      "metadata": {
        "id": "HE4w7zdWO_Gr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "xr3dkhUg2fiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79dd25d-98d4-46c5-9b10-efc7f2009210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2099444\n",
            "218808\n",
            "246993\n",
            "28913\n"
          ]
        }
      ],
      "source": [
        "class WordDict(object):\n",
        "    def __init__(self):\n",
        "      # mapping between word type to its index\n",
        "      self.word2idx = {}\n",
        "      # mapping between index to word type\n",
        "      self.idx2word = {}\n",
        "      self.next_id = 0 # default value\n",
        "      self.add_word(\"<sos>\")\n",
        "      self.add_word(\"<eos>\")\n",
        "\n",
        "    def add_word(self, word):\n",
        "      \"\"\"\n",
        "      Add word and id to word2ids and idx2word\n",
        "      \"\"\"\n",
        "      if word not in self.word2idx:\n",
        "        self.word2idx[word] = self.next_id\n",
        "        self.idx2word[self.next_id] = word\n",
        "        self.next_id += 1\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "      self.train_file = read_text_file_to_list(os.path.join(path, 'train.txt'))\n",
        "      self.valid_file = read_text_file_to_list(os.path.join(path, 'valid.txt'))\n",
        "      self.test_file = read_text_file_to_list(os.path.join(path, 'test.txt'))\n",
        "\n",
        "      self.dictionary = WordDict() \n",
        "      self.add_data_to_vocab()\n",
        "\n",
        "      self.train = self.tokenize(self.train_file)\n",
        "      self.valid = self.tokenize(self.valid_file)\n",
        "      self.test = self.tokenize(self.test_file)\n",
        "    \n",
        "    def add_data_to_vocab(self):\n",
        "      \"\"\"\n",
        "      Add tokens from train, validation, and test set to vocab\n",
        "      \"\"\"\n",
        "      for data in [self.train_file, self.valid_file, self.test_file]:\n",
        "        for line in data:\n",
        "          line = re.sub('\\s+',' ',line)\n",
        "          line = line.strip()\n",
        "          if line:\n",
        "            line = line.lower()\n",
        "            for token in line.split():\n",
        "              self.dictionary.add_word(token)\n",
        "              \n",
        "    def tokenize(self, text):\n",
        "        ################################\n",
        "        ## TODO: \n",
        "        ## (1) build vocabulary on three given files, using class WordDict \n",
        "        ## (2) tokenize each file content with the vocabulary, return a list of token ids\n",
        "        ## Note that in this implementation, we add words in validation and test file into the vocabulary,\n",
        "        ## so there is no unknown word.\n",
        "        ################################\n",
        "\n",
        "        all_token_ids = []\n",
        "\n",
        "        for line in text:\n",
        "          line = re.sub('\\s+',' ',line)\n",
        "          line = line.strip()\n",
        "          if line:\n",
        "            line = line.lower()\n",
        "            line_tokens = ['<sos>'] + line.split() + ['<eos>']\n",
        "            all_token_ids.extend([self.dictionary.word2idx[token] for token in line_tokens])\n",
        "        return all_token_ids\n",
        "\n",
        "corpus = Corpus(DATA_DIR)\n",
        "print(len(corpus.train))\n",
        "print(len(corpus.valid))\n",
        "print(len(corpus.test))\n",
        "print(len(corpus.dictionary))\n",
        "assert len(corpus.dictionary) == 28913\n",
        "assert len(corpus.train) == 2099444\n",
        "assert len(corpus.valid) == 218808\n",
        "assert len(corpus.test) == 246993"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "6of33pFT94dM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d51ac1-d6e2-42df-a23b-bdc7e4160007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20994, 100])\n",
            "torch.Size([2188, 100])\n",
            "torch.Size([2469, 100])\n"
          ]
        }
      ],
      "source": [
        "def batchify(ids, batch_size):\n",
        "    \"\"\"\n",
        "    batchify arranges the dataset into columns.\n",
        "    # Parameters\n",
        "    ids : Tensor\n",
        "        1-dimensional tensor of token ids\n",
        "    batch_size : Int\n",
        "        batch_size\n",
        "    # Returns\n",
        "    data: a torch.LongTensor with shape of (len(ids)//batch_size, batch_size)\n",
        "        batchified corpus data\n",
        "\n",
        "    For example, the input ids [1,2,3,4,5,6,7,8,9] and batch_size=2\n",
        "    output is:\n",
        "    [ [1, 5],\n",
        "      [2, 6],\n",
        "      [3, 7],\n",
        "      [4, 8] ]\n",
        "    The shape of the tensor is 4x2. \n",
        "    We trim off any extra elements (9 in this example) that wouldn't cleanly fit.\n",
        "    ***Again, note that the text order is in the column.***\n",
        "    \"\"\" \n",
        "    num_cols = math.floor(len(ids)/batch_size)\n",
        "    list_subsets = [ids[i:i+num_cols] for i in \n",
        "                    range(0, len(ids)-num_cols+1, num_cols)]\n",
        "\n",
        "    batchified = torch.transpose(torch.LongTensor(list_subsets),0,1)\n",
        "    return batchified\n",
        "    \n",
        "# print(batchify([1,2,3,4,5,6,7,8,9], 2))\n",
        "# print(batchify([1,2,3,4,5,6,7,8,9], 3))\n",
        "# print(batchify([1,2,3,4,5,6,7,8,9,10], 3))\n",
        "# print(batchify([1,2,3,4,5,6,7,8,9,10], 2))\n",
        "\n",
        "train_data = batchify(corpus.train, TRAIN_BATCH_SIZE)\n",
        "val_data = batchify(corpus.valid, TEST_BATCH_SIZE)\n",
        "test_data = batchify(corpus.test, TEST_BATCH_SIZE)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIYJzPZLQPbU",
        "outputId": "d1a8323c-bc32-41fd-b1ea-5f920bcef6a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,   701,    10,  ...,    18, 28809,   272],\n",
            "        [    2,  1791,    14,  ...,   438,  8623, 20553],\n",
            "        [    3,   130,   119,  ...,   984,    18,   300],\n",
            "        ...,\n",
            "        [  794,    18,  7030,  ...,   436,   407,    17],\n",
            "        [ 3536,  5282,    11,  ...,   843,   206,   157],\n",
            "        [ 1566,   725,  3917,  ...,    16,   290,   857]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "qhtQEbhR_hNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ae7c4f-8d92-4010-b3f0-dc1ddc3c6b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[1, 5],\n",
            "        [2, 6]]), tensor([2, 6, 3, 7]))\n",
            "(tensor([[3, 7]]), tensor([4, 8]))\n",
            "torch.Size([50, 100])\n",
            "torch.Size([5000])\n",
            "tensor([[    0,   701,    10,  ...,    18, 28809,   272],\n",
            "        [    2,  1791,    14,  ...,   438,  8623, 20553],\n",
            "        [    3,   130,   119,  ...,   984,    18,   300],\n",
            "        ...,\n",
            "        [   35,    17,  5419,  ...,  5099,    16,    14],\n",
            "        [   36,   346,    62,  ...,    14,     1,  1625],\n",
            "        [   37,  3544,    38,  ...,  7773,     0,  1654]])\n"
          ]
        }
      ],
      "source": [
        "def get_batch(source, i, bptt = BPTT):\n",
        "    \"\"\"\n",
        "    # Parameters\n",
        "    source : Tensor\n",
        "        corpus as 2-dimensional tensor\n",
        "    i : Int\n",
        "        minibatch index\n",
        "\n",
        "    # Returns\n",
        "    data : 2D tensor \n",
        "        LSTM input\n",
        "    target : 1D tensor\n",
        "        LSTM output target\n",
        "\n",
        "    Consider the following example where \"source\" is a 2d tensor of shape (4, 2).\n",
        "    In this example we have 4 batches, each of size 2.\n",
        "    [ [1, 5],\n",
        "      [2, 6],\n",
        "      [3, 7],\n",
        "      [4, 8] ]\n",
        "\n",
        "    Suppose we set BPTT (backpropagation through time, see A3 pdf for details) to 2.\n",
        "    At index i = 0, the input to our LSTM becomes:\n",
        "    [ [1, 5],\n",
        "      [2, 6] ]\n",
        "    This corresponds to the first 2 batches in the sequence.\n",
        "    The target would correspondingly be (again, since BPTT is 2): \n",
        "    [ [2, 6],\n",
        "      [3, 7] ]\n",
        "    However, we need to reshape it from 2-dimensions to 1-dimension:\n",
        "    [2, 6, 3, 7]\n",
        "\n",
        "    For the next batch, index i = prev_i + BPTT = 2. \n",
        "    However, i + BPTT = 2 + 2 = 4 and 4 >= len(source). This wouldn't work.\n",
        "    So, to account for this edge case, we consider BPTT to be:\n",
        "    len(source) - 1 - i = 4 - 1 - 2 = 1\n",
        "    As such, our input now becomes:\n",
        "    [ [3, 7] ]\n",
        "    and target is \n",
        "    [4, 8]. \n",
        "    \"\"\" \n",
        "    ###################################################\n",
        "    # TODO Assign these variables to the right values #\n",
        "    ###################################################\n",
        "    seq_len =  (len(source) - 1 - i) if (i + bptt  >= len(source)) else bptt\n",
        "    data = source[i:i+seq_len]\n",
        "    target = torch.flatten(source[i+1:i+1+seq_len])\n",
        "    return data, target\n",
        "\n",
        "\n",
        "test = batchify([1,2,3,4,5,6,7,8,9], 2)\n",
        "print(get_batch(test, i=0, bptt=2))\n",
        "print(get_batch(test, i=2, bptt=2))\n",
        "\n",
        "data, targets = get_batch(train_data, 0)\n",
        "print(data.shape)\n",
        "print(targets.shape)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "6C8QNsUL9i1S"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "## TODO: Implement RNN LSTM\n",
        "## documentation of pytorch LSTM interface: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "################################\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, word_embedding_size, nhid, nlayers, dropout=0.5, tied_weights=False):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.nhid = nhid # hidden dimension of LSTM\n",
        "        self.nlayers = nlayers # number of LSTM layers\n",
        "        # TODO: initialize the required modules for the LSTM model\n",
        "        # HINT: batch_first should be False in LSTM since our data structure is not batch first.\n",
        "        self.encoder = self.embedding = torch.nn.Embedding(vocab_size, word_embedding_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.lstm = nn.LSTM(input_size = word_embedding_size, \n",
        "                          hidden_size = self.nhid, \n",
        "                          num_layers = self.nlayers,\n",
        "                          batch_first = False)\n",
        "        self.decoder = nn.Linear(self.nhid, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        For example:\n",
        "        # initrange = 0.1\n",
        "        # nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        This is not all that you need!\n",
        "        \"\"\"\n",
        "        # TODO: initialize the parameters\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)   \n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)   \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"\n",
        "        # Parameters\n",
        "        input: input embedding\n",
        "        hidden: hidden states in LSTM\n",
        "        # Returns\n",
        "        decoded: refers to the output of decoder layer over the vocabulary. Note that you don't need to pass it through the softmax layer\n",
        "        hidden: stores the hidden states in LSTM\n",
        "        \"\"\"\n",
        "        embeds = self.embedding(input)\n",
        "        out = self.dropout(embeds)\n",
        "        out, hidden = self.lstm(out, hidden)\n",
        "        out = self.dropout(embeds)\n",
        "        decoded = self.decoder(out).flatten(0,1)\n",
        "        return decoded, hidden\n",
        "\n",
        "    # initialize parameters in LSTM\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "            weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "hVpQfjuxOm0Y"
      },
      "outputs": [],
      "source": [
        "# Set the random seed for reproducibility.\n",
        "torch.manual_seed(SEED)\n",
        "# set device as GPU/CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "steNXz9I9iqI",
        "outputId": "f84df197-7cdf-4560-e34d-1dcbeff6765b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "xZT5ZAu7EX3v"
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "\n",
        "    hidden = model.init_hidden(TRAIN_BATCH_SIZE)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, BPTT)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # print(torch.max(data))\n",
        "        # print(\"data shape: \", data.shape)\n",
        "        # print(\"targets shape: \", targets.shape)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        hidden = repackage_hidden(hidden) # Note that the main advantage here is that the hidden value is continual from the previous forward pass\n",
        "        output, hidden = model(data, hidden)\n",
        "        # print(\"output shape: \", output.shape)\n",
        "        # print(\"target shape: \", targets.shape)\n",
        "        # print(\"output type: \", type(output))\n",
        "        # print(\"target type: \", type(targets))\n",
        "        loss = criterion(output.to(torch.float), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
        "            cur_loss = total_loss / LOG_INTERVAL\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // BPTT,\n",
        "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "aTsdN6rLG0fc"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute the loss of model on data_source\n",
        "def evaluate(data_source, expected_size = BPTT ):\n",
        "    model.eval()\n",
        "    # TODO: get the average negative log likelihood on the data_source\n",
        "    # return average_log_loss\n",
        "    epoch_loss, epoch_acc = 0, 0\n",
        "    epoch_tp, epoch_fp, epoch_fn = 0, 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      hidden = model.init_hidden(TEST_BATCH_SIZE)\n",
        "      batch_starts = range(0, data_source.size(0) - 1, BPTT)\n",
        "      for batch, i in enumerate(batch_starts):\n",
        "        data, targets = get_batch(data_source, i)\n",
        "        data = data.to(device)\n",
        "        batch_size = data.shape[0]\n",
        "        targets = targets.to(device)\n",
        "        out, hidden = model.forward(data, hidden)\n",
        "        loss = criterion(out.to(torch.float), targets)\n",
        "        loss_weight = batch_size/expected_size # downweight shorter sequence?\n",
        "        epoch_loss += ((loss_weight*loss.item())/len(batch_starts))\n",
        "    print(\"epoch loss: \", epoch_loss)\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "MYuaQ5cSHxWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d685141-bc6b-487a-802d-f68817516568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   100/  419 batches | ms/batch 181.49 | loss  9.92 | ppl 20284.83\n",
            "| epoch   1 |   200/  419 batches | ms/batch 179.67 | loss  7.91 | ppl  2732.76\n",
            "| epoch   1 |   300/  419 batches | ms/batch 180.99 | loss  7.01 | ppl  1108.00\n",
            "| epoch   1 |   400/  419 batches | ms/batch 182.87 | loss  6.64 | ppl   763.68\n",
            "epoch loss:  6.138001237782566\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 79.04s | valid loss  6.14 | valid ppl   463.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   2 |   100/  419 batches | ms/batch 184.32 | loss  6.42 | ppl   614.58\n",
            "| epoch   2 |   200/  419 batches | ms/batch 182.73 | loss  6.24 | ppl   510.83\n",
            "| epoch   2 |   300/  419 batches | ms/batch 182.56 | loss  6.15 | ppl   468.07\n",
            "| epoch   2 |   400/  419 batches | ms/batch 182.26 | loss  6.08 | ppl   435.83\n",
            "epoch loss:  5.782747320478613\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 79.67s | valid loss  5.78 | valid ppl   324.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   3 |   100/  419 batches | ms/batch 183.91 | loss  6.05 | ppl   426.15\n",
            "| epoch   3 |   200/  419 batches | ms/batch 182.50 | loss  5.95 | ppl   384.81\n",
            "| epoch   3 |   300/  419 batches | ms/batch 182.61 | loss  5.91 | ppl   368.54\n",
            "| epoch   3 |   400/  419 batches | ms/batch 182.21 | loss  5.87 | ppl   354.73\n",
            "epoch loss:  5.653856618404388\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 79.63s | valid loss  5.65 | valid ppl   285.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   4 |   100/  419 batches | ms/batch 184.52 | loss  5.89 | ppl   361.69\n",
            "| epoch   4 |   200/  419 batches | ms/batch 182.63 | loss  5.81 | ppl   333.71\n",
            "| epoch   4 |   300/  419 batches | ms/batch 182.47 | loss  5.78 | ppl   323.00\n",
            "| epoch   4 |   400/  419 batches | ms/batch 182.91 | loss  5.75 | ppl   313.40\n",
            "epoch loss:  5.584781746430831\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 79.77s | valid loss  5.58 | valid ppl   266.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   5 |   100/  419 batches | ms/batch 184.24 | loss  5.79 | ppl   326.49\n",
            "| epoch   5 |   200/  419 batches | ms/batch 182.35 | loss  5.72 | ppl   304.14\n",
            "| epoch   5 |   300/  419 batches | ms/batch 182.64 | loss  5.69 | ppl   295.32\n",
            "| epoch   5 |   400/  419 batches | ms/batch 182.26 | loss  5.66 | ppl   287.70\n",
            "epoch loss:  5.5417218522592036\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 79.66s | valid loss  5.54 | valid ppl   255.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   6 |   100/  419 batches | ms/batch 184.40 | loss  5.71 | ppl   303.04\n",
            "| epoch   6 |   200/  419 batches | ms/batch 182.27 | loss  5.65 | ppl   283.52\n",
            "| epoch   6 |   300/  419 batches | ms/batch 182.39 | loss  5.62 | ppl   274.88\n",
            "| epoch   6 |   400/  419 batches | ms/batch 182.62 | loss  5.59 | ppl   268.61\n",
            "epoch loss:  5.510238793763247\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 79.69s | valid loss  5.51 | valid ppl   247.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   7 |   100/  419 batches | ms/batch 184.33 | loss  5.65 | ppl   285.04\n",
            "| epoch   7 |   200/  419 batches | ms/batch 182.67 | loss  5.59 | ppl   267.44\n",
            "| epoch   7 |   300/  419 batches | ms/batch 182.73 | loss  5.56 | ppl   259.94\n",
            "| epoch   7 |   400/  419 batches | ms/batch 182.95 | loss  5.54 | ppl   254.30\n",
            "epoch loss:  5.4876914178241405\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 79.76s | valid loss  5.49 | valid ppl   241.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   8 |   100/  419 batches | ms/batch 184.27 | loss  5.60 | ppl   270.47\n",
            "| epoch   8 |   200/  419 batches | ms/batch 182.69 | loss  5.54 | ppl   254.56\n",
            "| epoch   8 |   300/  419 batches | ms/batch 182.63 | loss  5.51 | ppl   246.91\n",
            "| epoch   8 |   400/  419 batches | ms/batch 182.31 | loss  5.49 | ppl   241.78\n",
            "epoch loss:  5.469969584725117\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 79.69s | valid loss  5.47 | valid ppl   237.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch   9 |   100/  419 batches | ms/batch 183.76 | loss  5.56 | ppl   258.58\n",
            "| epoch   9 |   200/  419 batches | ms/batch 182.37 | loss  5.50 | ppl   243.63\n",
            "| epoch   9 |   300/  419 batches | ms/batch 182.33 | loss  5.47 | ppl   236.62\n",
            "| epoch   9 |   400/  419 batches | ms/batch 182.75 | loss  5.45 | ppl   231.81\n",
            "epoch loss:  5.45611549204046\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 79.65s | valid loss  5.46 | valid ppl   234.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n",
            "| epoch  10 |   100/  419 batches | ms/batch 185.04 | loss  5.51 | ppl   248.24\n",
            "| epoch  10 |   200/  419 batches | ms/batch 182.94 | loss  5.46 | ppl   234.65\n",
            "| epoch  10 |   300/  419 batches | ms/batch 182.64 | loss  5.43 | ppl   227.46\n",
            "| epoch  10 |   400/  419 batches | ms/batch 182.97 | loss  5.41 | ppl   223.44\n",
            "epoch loss:  5.445385703607037\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 79.88s | valid loss  5.45 | valid ppl   231.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "save new best model!\n"
          ]
        }
      ],
      "source": [
        "# prepare the model, loss, and optimizer\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = LSTMModel(ntokens, WORD_EMBED_DIM, HID_EMBED_DIM, N_LAYERS, DROPOUT, TIED).to(device)\n",
        "criterion = nn.CrossEntropyLoss() # use crossentropy loss\n",
        "optimizer = torch.optim.Adam(model.parameters()) # use adam optimizer with default setting\n",
        "best_val_loss = None\n",
        "\n",
        "# Training framework\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "        val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    \n",
        "    # Save the model if the validation loss is the best we've seen so far.\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        with open(SAVE_BEST, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "            print(\"save new best model!\")\n",
        "        best_val_loss = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "1XxYhaWUicWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec665681-3b09-449f-d48b-f12ebb6656f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch loss:  5.341324213409423\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.34 | test ppl   208.79\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load the best saved model.\n",
        "with open(SAVE_BEST, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # After loading the RNN params, they are not a continuous chunk of memory.\n",
        "    # flatten_paramters() makes them a continuous chunk, and will speed up the forward pass.\n",
        "    # Currently, only RNN model supports flatten_parameters function.\n",
        "    model.lstm.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "YmzWgouuHhwF"
      },
      "outputs": [],
      "source": [
        "# Generation with GPT-2\n",
        "# Check this tutorial: https://huggingface.co/blog/how-to-generate\n",
        "# It comes with a notebook. You need to run through that notebook and understand different sampling procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "aKmS7WvIHhwG"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, sampling_func):\n",
        "    # # Generation with LSTM lm given a sampling function and a prompt\n",
        "    max_length = 30\n",
        "    ids = []\n",
        "    for word in prompt.split():\n",
        "        ids.append(corpus.dictionary.word2idx[word])\n",
        "    hidden = model.init_hidden(1)\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        output, hidden = model(torch.LongTensor([[wid] for wid in ids]).to(device), hidden)\n",
        "        word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "        generations = []\n",
        "        for i in range(max_length):\n",
        "            word_idx = sampling_func(word_prob)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            generations.append(word)\n",
        "            if word == \"<eos>\":\n",
        "                break\n",
        "            new_word = torch.LongTensor([[word_idx]]).to(device)\n",
        "            output, hidden = model(new_word, hidden)\n",
        "            word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "    return generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "MnEmUWiwHhwG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def greedy_sampling(word_prob):\n",
        "    # TODO: return the word with the max probability\n",
        "    return torch.argmax(word_prob).item()\n",
        "\n",
        "def random_sampling(word_prob):\n",
        "    # TODO: sample a random word based on the probability vector\n",
        "    # weights are cumulative\n",
        "    id = random.choices([x for x in range(0,word_prob.shape[0])], cum_weights=word_prob.tolist(), k=1)[0]\n",
        "    return id\n",
        "\n",
        "def topk_sampling(word_prob, k=10):\n",
        "    probs, indices = torch.topk(word_prob, k)\n",
        "    # weights are relative\n",
        "    id = random.choices(indices.tolist(), weights=probs.tolist(), k=1)[0]\n",
        "    return id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "XpCGmlnTHhwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f76ced-48b3-4384-c56c-211b9a48bd1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i went to\n",
            "the <unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"i went to\".lower()\n",
        "generations = generate_text(prompt, greedy_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"i went to\".lower()\n",
        "generations = generate_text(prompt, random_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ],
      "metadata": {
        "id": "18A0eY-2sINH",
        "outputId": "6db93ce0-d15a-4010-aab0-b707dae21047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i went to\n",
            "<sos> t tristan <sos> t <sos> <sos> t tristan schmoke <sos> <sos> <sos> t civic <sos> <sos> <sos> t <sos> <sos> t <sos> t civic painting <sos> t tristan advisers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"i went to\".lower()\n",
        "generations = generate_text(prompt, topk_sampling) # replace sample_func with the sampling function that you would like to try\n",
        "print('prompt: ' + prompt)\n",
        "print(' '.join(generations))"
      ],
      "metadata": {
        "id": "MRyyllxjsIV8",
        "outputId": "2279bf2f-66a6-4dc7-de44-6f3cec23955d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i went to\n",
            "a large number of the united states , but was the game , <unk> , <unk> . <eos>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "a3_lm_template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}